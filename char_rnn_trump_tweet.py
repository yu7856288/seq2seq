# -*-encoding=utf-8
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
import random
import sys
sys.path.append('..')
import time
import tensorflow as tf
import utils

def vocab_encode(text, vocab):
    return [vocab.index(x) + 1 for x in text if x in vocab] ####å¯¹vocabå¯¹å•è¯åšindexï¼Œä»1å¼€å§‹è®¡æ•°,0æ˜¯ç•™ç©ºï¼Œç”¨äºè¡¥å……0


def vocab_decode(array, vocab):
    return ''.join([vocab[x - 1] for x in array]) ####æŠŠindexè½¬æ¢ä¸ºå•è¯ï¼Œindexéœ€è¦å‡1


def read_data(filename, vocab, sequence_length, overlap):
    lines = [line.strip() for line in open(filename, 'r', encoding='utf-8').readlines()]
    while True:
        random.shuffle(lines)
        for text in lines:
            text = vocab_encode(text, vocab)####è½¬ä¸ºåºå·
            for start in range(0, len(text) - sequence_length, overlap):
                chunk = text[start: start + sequence_length]
                chunk += [0] * (sequence_length - len(chunk))###ä¸æ»¡sequence_length sizeçš„chunk åˆ™è¡¥0
                yield chunk


def read_batch(stream, batch_size):
    batch = []
    for element in stream: ####for in æœ‰nextåŠŸèƒ½
        batch.append(element)
        if len(batch) == batch_size:
            yield batch ###yieldè¿”å›ç»“æœ
            batch = [] ###ä»æ­¤å¤„æ‰§è¡Œæ–°çš„è¯­å¥
    yield batch


class CharRNN(object):
    def __init__(self, model):
        self.model = model
        self.path = 'data/' + model + '.txt'
        self.temp = tf.constant(1.5)
        self.hidden_sizes = [128, 256]
        self.batch_size = 64
        self.lr = 0.0003
        self.skip_step = 100  ###è¯„ä¼°é—´éš”
        self.sequence_length = 50  # æ—¶é—´æ­¥ é•¿åº¦
        self.len_generated = 200  ####ç”Ÿæˆçš„å¥å­çš„é•¿åº¦
        self.gstep = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step') ####è®¡æ•°å™¨

        #####è·å¾—å­—å…¸vocab å…±87ä¸ª
        if 'trump' in model:
            self.vocab = ("$%'()+,-./0123456789:;=?ABCDEFGHIJKLMNOPQRSTUVWXYZ"
                          " '\"_abcdefghijklmnopqrstuvwxyz{|}@#â¡ğŸ“ˆ")
        else:
            self.vocab = (" $%'()+,-./0123456789:;=?ABCDEFGHIJKLMNOPQRSTUVWXYZ"
                          "\\^_abcdefghijklmnopqrstuvwxyz{|}")

        self.seq = tf.placeholder(tf.int32, [None, None]) ###è¾“å…¥batch  0ç»´å¯èƒ½ä¸è¶³batch size x seqence_length


    def create_rnn(self, seq):
        layers = [tf.nn.rnn_cell.GRUCell(size) for size in self.hidden_sizes] ####å †å  GRUcell ç”¨ä¸¤å±‚
        cells = tf.nn.rnn_cell.MultiRNNCell(layers)
        batch = tf.shape(seq)[0]
        zero_states = cells.zero_state(batch, dtype=tf.float32)
        self.in_state = tuple([tf.placeholder_with_default(state, [None, state.shape[1]])
                               for state in zero_states])###  ç¬¬ä¸€ç»´ä¸ºNone ï¼Œå¯ä½¿ç”¨ä¸åŒçš„batch å¤§å° å°†list è½¬tuple,stateéœ€è¦tuple

        # this line to calculate the real length of seq
        # all seq are padded to be of the same length, which is sequence_length
        length = tf.reduce_sum(tf.reduce_max(tf.sign(seq), 2), 1)####zè¿™è¡Œä»£ç å¯èƒ½æœ‰è¯¯
        self.output, self.out_state = tf.nn.dynamic_rnn(cells, seq, length, self.in_state) ###outputä¸GRUçš„éšå«å€¼çš„hä¸€è‡´
        ###output shape [64,50,256]

    def create_model(self):
        seq = tf.one_hot(self.seq, len(self.vocab)) ###æŠŠå­—å…¸è½¬ä¸ºone hot ç¼–ç 
        #seq shape [64,50,87]
        self.create_rnn(seq)
        ###gruè¾“å‡ºå±‚å’Œéšå«hä¸€è‡´ï¼Œéœ€è¦åštransformï¼Œç”¨tf.layers.denseåšç»´åº¦å˜æ¢
        self.logits = tf.layers.dense(self.output, len(self.vocab), activation=None)###output åšä¸€ä¸ªdenseçš„transformï¼Œè¾“å‡ºçš„unitsä¸ªæ•°ä¸ºå­—å…¸é•¿åº¦ï¼Œä¸‹ä¸€æ­¥åšsoftMax
        ####logits shape [64,50,87]
        loss = tf.nn.softmax_cross_entropy_with_logits(logits=self.logits[:, :-1], #### é¢„æµ‹åºåˆ—ï¼Œy0,y1,...y49,å› ä¸ºæœ€åä¸€ä¸ªä»x49å–å¾—ï¼Œåˆ™ä¸éœ€è¦è¿™ä¸ªy49
                                                       labels=seq[:, 1:]) ####labelsæ˜¯è¾“å‡ºçš„åºåˆ—ï¼ŒåŸåºåˆ—æ˜¯x0ï¼Œx1ï¼Œx2,...,åˆ™é¢„æµ‹åºåˆ—ç»´x1,x2,x3,...
        self.loss = tf.reduce_sum(loss)
        # sample the next character from Maxwell-Boltzmann Distribution
        # with temperature temp. It works equally well without tf.exp
        self.sample = tf.multinomial(tf.exp(self.logits[:, -1] / self.temp), 1)[:, 0] ###output Maxwell-Boltzmann Distribution é‡‡æ ·å¾—åˆ°ä¸€ä¸ªsample
        ###sample shape [64,1] æˆ–è€…å°±æ˜¯[64]

        self.opt = tf.train.AdamOptimizer(self.lr).minimize(self.loss, global_step=self.gstep)

    def train(self):
        saver = tf.train.Saver()
        start = time.time()
        min_loss = None
        with tf.Session() as sess:
            writer = tf.summary.FileWriter('graphs/gist', sess.graph)
            sess.run(tf.global_variables_initializer())

            ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/' + self.model + '/checkpoint'))
            if ckpt and ckpt.model_checkpoint_path:
                saver.restore(sess, ckpt.model_checkpoint_path)

            iteration = self.gstep.eval()
            stream = read_data(self.path, self.vocab, self.sequence_length, overlap=self.sequence_length // 2)
            data = read_batch(stream, self.batch_size)
            while True:
                batch = next(data)
                ###batch shape [64,50]
                output_,logit_=sess.run([self.output,self.logits],feed_dict={self.seq:batch})
                # for batch in read_batch(read_data(DATA_PATH, vocab)):
                batch_loss, _ = sess.run([self.loss, self.opt], {self.seq: batch})
                if (iteration + 1) % self.skip_step == 0:
                    print('Iter {}. \n    Loss {}. Time {}'.format(iteration, batch_loss, time.time() - start))
                    self.online_infer(sess)
                    start = time.time()
                    checkpoint_name = 'checkpoints/' + self.model + '/char-rnn'
                    if min_loss is None:
                        saver.save(sess, checkpoint_name, iteration)
                    elif batch_loss < min_loss:
                        saver.save(sess, checkpoint_name, iteration)
                        min_loss = batch_loss
                iteration += 1

    def online_infer(self, sess):
        """ Generate sequence one character at a time, based on the previous character
        """
        for seed in ['Hillary', 'I', 'R', 'T', '@', 'N', 'M', '.', 'G', 'A', 'W']:
            sentence = seed
            state = None
            for _ in range(self.len_generated):
                batch = [vocab_encode(sentence[-1], self.vocab)] ###å–å¾—seedçš„æœ€åä¸€ä¸ªå­—æ¯ï¼Œç¼–ç  å¤–å¥—ä¸€å±‚ï¼Œåˆ™batchä¸ºä¸¤ç»´
                 ##æ­¤å¤„batch [1,1]
                feed = {self.seq: batch}
                if state is not None:  # for the first decoder step, the state is None
                    for i in range(len(state)):
                        feed.update({self.in_state[i]: state[i]})
                index, state,logits_= sess.run([self.sample, self.out_state,self.logits], feed) ####æ‰§è¡Œä¸€æ¬¡ï¼Œå¾—åˆ°ä¸€ä¸ªsampleï¼Œéœ€è¦æ›´æ–°in_state,å› ä¸ºè¿™ä¸ªæ˜¯åºåˆ—ç”Ÿæˆï¼Œåªèƒ½ä¸€æ­¥ä¸€æ­¥è·‘ï¼Œæ²¡æ³•ç”¨dynamic_rnn
                print(logits_.shape)
                sentence += vocab_decode(index, self.vocab)
            print('\t' + sentence)


def main():
    model = 'trump_tweets'
    utils.safe_mkdir('checkpoints')
    utils.safe_mkdir('checkpoints/' + model)
    lm = CharRNN(model)
    lm.create_model()
    lm.train()


if __name__ == '__main__':
    main()
